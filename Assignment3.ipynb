{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        dataset = pickle.load(f, encoding='latin1') # Nxd(3072) (Nx (32x32x3))\n",
    "        X = np.transpose(dataset['data'] / 255.) # d x N\n",
    "        mean_X = np.mean(X, axis=1) # mean of each row (each feature mean)\n",
    "        std_X = np.std(X, axis=1)\n",
    "        X = X - np.matlib.repmat(mean_X, X.shape[1], 1).T\n",
    "        X = np.divide(X, np.matlib.repmat(std_X, X.shape[1], 1).T)\n",
    "        \n",
    "        y = np.array(dataset['labels'])\n",
    "        Y = np.transpose(np.eye(X.shape[1], np.max(y) + 1)[y]) # K x N\n",
    "        return X, Y, y\n",
    "\n",
    "def load_all(validation_size):\n",
    "    X_1, Y_1, y_1 = load_batch('data/data_batch_1')\n",
    "    X_2, Y_2, y_2 = load_batch('data/data_batch_2')\n",
    "    X_3, Y_3, y_3 = load_batch('data/data_batch_3')\n",
    "    X_4, Y_4, y_4 = load_batch('data/data_batch_4')\n",
    "    X_5, Y_5, y_5 = load_batch('data/data_batch_5')\n",
    "    \n",
    "    X = np.concatenate((X_1, X_2, X_3, X_4, X_5[:,:-validation_size]), axis=1)\n",
    "    Y = np.concatenate((Y_1, Y_2, Y_3, Y_4, Y_5[:,:-validation_size]), axis=1)\n",
    "    y = np.concatenate((y_1, y_2, y_3, y_4, y_5[:-validation_size]))\n",
    "    \n",
    "    X_valid = X_5[:,-validation_size:]\n",
    "    Y_valid = Y_5[:,-validation_size:]\n",
    "    y_valid = y_5[-validation_size:]\n",
    "    return X, Y, y, X_valid, Y_valid, y_valid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalize(scores, mean, variance):\n",
    "#     return np.dot(np.power(np.diag(variance) + 1e-9, -0.5), (scores - np.array([mean])))\n",
    "    return (scores - mean.reshape((-1,1))) / np.sqrt(variance.reshape((-1,1)) + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(s):\n",
    "    exponent = np.exp(s)\n",
    "    return np.divide(exponent, np.sum(exponent, axis=0))\n",
    "\n",
    "def evaluate_classifier(X, layers, batch_norm=False, precomp=False):\n",
    "    num_layers = len(layers)\n",
    "    H = []\n",
    "    S = []\n",
    "    S_norm = []\n",
    "    \n",
    "    layer_means = []\n",
    "    layer_variances = []\n",
    "    \n",
    "    gammas = []\n",
    "    betas = []\n",
    "    \n",
    "    h_prev = X\n",
    "    \n",
    "    for i, layer in enumerate(layers):\n",
    "        if i == num_layers - 1:  # If last layer\n",
    "            P = softmax(np.dot(layer[\"W\"], h_prev) + layer[\"b\"]) # K x N\n",
    "            \n",
    "            if batch_norm:\n",
    "                return H, P, S, S_norm, np.array(layer_means), np.array(layer_variances), gammas, betas\n",
    "            else:\n",
    "                return H, P\n",
    "        else:\n",
    "            s = np.dot(layer[\"W\"], h_prev) + layer[\"b\"] # m x N\n",
    "            \n",
    "            if batch_norm:\n",
    "                S.append(s)\n",
    "                \n",
    "                if precomp:\n",
    "                    print(\"USING PRECOMPUTED MEANS\")\n",
    "                    mean = layer[\"mean\"]\n",
    "                    variance = layer[\"var\"]\n",
    "                    print(mean)\n",
    "                else:\n",
    "                    mean = np.mean(s, axis=1) # m len\n",
    "#                     variance = np.mean(np.square(s - np.array([mean]).transpose()), axis=1) # m\n",
    "                    variance = np.var(s, axis=1)\n",
    "                    layer_means.append(mean)\n",
    "                    layer_variances.append(variance)\n",
    "\n",
    "                gamma = layer[\"gamma\"]\n",
    "                beta = layer[\"beta\"]\n",
    "                \n",
    "                gammas.append(gamma)\n",
    "                betas.append(beta)\n",
    "                \n",
    "                normalized = batch_normalize(s, mean, variance)\n",
    "                S_norm.append(normalized)\n",
    "                \n",
    "                transformed = np.multiply(gamma, normalized) + beta\n",
    "                h = np.maximum(transformed, 0)\n",
    "                H.append(h)\n",
    "                h_prev = h\n",
    "            else:\n",
    "                h = np.maximum(s, 0) # ReLU; m x N\n",
    "                H.append(h)\n",
    "                h_prev = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_backpass(G, S, mean, variance, Nb):\n",
    "    sigma_1 = np.array([np.power(v + 1e-9,-0.5) for v in variance]).reshape((-1,1))\n",
    "    sigma_2 = np.array([np.power(v + 1e-9,-1.5) for v in variance]).reshape((-1,1))\n",
    "    \n",
    "    G_1 = np.multiply(G, np.dot(sigma_1, np.ones((Nb, 1)).T))\n",
    "    G_2 = np.multiply(G, np.dot(sigma_2, np.ones((Nb, 1)).T))\n",
    "    D = S - (np.dot(mean.reshape((-1,1)), np.ones((Nb, 1)).T))\n",
    "\n",
    "    c = np.multiply(G_2, D)\n",
    "    c = np.dot(c, np.ones((Nb, 1)))\n",
    "    \n",
    "    res = G_1 - np.divide(np.dot(np.dot(G_1, np.ones((Nb, 1))), np.ones((Nb, 1)).T), Nb) \n",
    "    res = res - np.divide(np.multiply(D, np.dot(c, np.ones((Nb, 1)).T)), Nb)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, Y, layers, lmb, batch_norm=False, precomp=False):\n",
    "    H, P = evaluate_classifier(X, layers, batch_norm, precomp)[:2]\n",
    "    n = np.sum(np.multiply(Y, P), axis=0)\n",
    "    cross_entropy = np.sum(-np.log(n))\n",
    "    \n",
    "    w_square_sum = 0\n",
    "    if lmb > 0:\n",
    "        for layer in layers:\n",
    "            w_square_sum += np.sum(np.diag(np.dot(layer[\"W\"].T, layer[\"W\"])))\n",
    "    return (cross_entropy / X.shape[1]) + (lmb * w_square_sum)\n",
    "\n",
    "def compute_gradients(X, Y, layers, lmb, batch_norm, average_mean=None, average_variance=None):\n",
    "    if batch_norm:\n",
    "        H, P, S, S_norm, layer_means, layer_variances, gammas, betas = evaluate_classifier(X, layers, batch_norm)\n",
    "    else:\n",
    "        H, P = evaluate_classifier(X, layers)\n",
    "\n",
    "    G = -(Y - P) # through loss and softmax\n",
    "    Nb = X.shape[1] # batch size\n",
    "    \n",
    "    W_gradients = []\n",
    "    b_gradients = []\n",
    "    \n",
    "    if batch_norm:\n",
    "        gamma_gradients = []\n",
    "        beta_gradients = []\n",
    "        alpha = 0.9\n",
    "        if average_mean is None and average_variance is None:\n",
    "            average_mean = layer_means\n",
    "            average_variance = layer_variances\n",
    "        else:\n",
    "            average_mean = (alpha * average_mean) + ((1. - alpha) * layer_means)\n",
    "            average_variance = (alpha * average_variance) + ((1. - alpha) * layer_variances)\n",
    "        \n",
    "        for i, layer in reversed(list(enumerate(layers))): # from last to first\n",
    "            if batch_norm and i == len(layers) - 1: # last\n",
    "                grad_W = np.divide(np.dot(G, H[i - 1].T), Nb) + (2 * lmb * layer[\"W\"]) # J w.r.t W_k\n",
    "                grad_b = np.divide(np.dot(G, np.ones((Nb, 1))), Nb) # J w.r.t b_k\n",
    "                G = np.dot(layer[\"W\"].T, G)\n",
    "                G = G * (H[i - 1] > 0).astype(int) # element-wise\n",
    "            else:\n",
    "                mp = np.multiply(G, S_norm[i])\n",
    "                gamma_grad = np.dot(np.divide(mp, Nb), np.ones((Nb, 1))) # J w.r.t gamma\n",
    "                print(gamma_grad.shape)\n",
    "                beta_grad = np.divide(np.dot(G, np.ones((Nb, 1))), Nb) # J w.r.t beta\n",
    "                print(beta_grad.shape)\n",
    "                \n",
    "                gamma_gradients.append(gamma_grad)\n",
    "                beta_gradients.append(beta_grad)\n",
    "\n",
    "                G = np.multiply(G, gammas[i] * np.ones((Nb, 1)).T) # eq.26\n",
    "\n",
    "                G = batch_norm_backpass(G, S[i], layer_means[i], layer_variances[i], Nb)\n",
    "\n",
    "                if i > 0: # prop back if not first\n",
    "                    grad_W = np.divide(np.dot(G, H[i - 1].T), Nb) + (2 * lmb * layer[\"W\"]) # J w.r.t W_k\n",
    "                    grad_b = np.divide(np.dot(G, np.ones((Nb, 1))), Nb) # J w.r.t b_k\n",
    "                    G = np.dot(layer[\"W\"].T, G)\n",
    "                    G = G * (H[i - 1] > 0).astype(int) # element-wise\n",
    "                else: # first\n",
    "                    grad_W = np.divide(np.dot(G, X.T), Nb) + (2 * lmb * layer[\"W\"]) # J w.r.t W_k\n",
    "                    grad_b = np.divide(np.dot(G, np.ones((Nb, 1))), Nb) # J w.r.t b_k\n",
    "\n",
    "            W_gradients.append(grad_W)\n",
    "            b_gradients.append(grad_b)\n",
    "            \n",
    "        return W_gradients, b_gradients, gamma_gradients, beta_gradients, average_mean, average_variance\n",
    "    else:\n",
    "        for i, layer in reversed(list(enumerate(layers))): # from last to first\n",
    "            if i > 0:\n",
    "                grad_W = np.divide(np.dot(G, H[i - 1].T), Nb) + (2 * lmb * layer[\"W\"]) # J w.r.t W_k\n",
    "                grad_b = np.divide(np.dot(G, np.ones((Nb, 1))), Nb) # J w.r.t b_k\n",
    "                G = np.dot(layer[\"W\"].T, G)\n",
    "                G = G * (H[i - 1] > 0).astype(int) # element-wise\n",
    "\n",
    "                W_gradients.append(grad_W)\n",
    "                b_gradients.append(grad_b)\n",
    "            else: # first layer\n",
    "                grad_W = np.divide(np.dot(G, X.T), Nb) + (2 * lmb * layer[\"W\"])\n",
    "                grad_b = np.divide(np.dot(G, np.ones((Nb, 1))), Nb)\n",
    "                W_gradients.append(grad_W)\n",
    "                b_gradients.append(grad_b)\n",
    "    \n",
    "        return W_gradients, b_gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(X, y, layers, batch_norm=False, precomp=False):\n",
    "    _, p = evaluate_classifier(X, layers, batch_norm, precomp)[:2]\n",
    "    argmax = np.argmax(p, axis=0) # max element index of each column\n",
    "    diff = argmax - y\n",
    "    return (diff == 0).sum() / X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_GD(X, Y, GDparams, layers, lmb, validation, batch_norm=False, calculate_loss=False):\n",
    "#     print(\"Training samples: {}\".format(X.shape[1]))\n",
    "#     print(\"Validation samples: {}\".format(validation[\"X\"].shape[1]))\n",
    "#     print(\"Training parameters: \", GDparams)\n",
    "    J_training = []\n",
    "    J_validation = []\n",
    "    \n",
    "    eta_diff = GDparams[\"eta_max\"] - GDparams[\"eta_min\"]\n",
    "    eta = GDparams[\"eta_min\"]\n",
    "    t = 0 # step\n",
    "    l = 0 # cycle\n",
    "    \n",
    "    average_mean = None\n",
    "    average_variance = None\n",
    "    \n",
    "    runs_in_epoch = int(X.shape[1] / GDparams[\"n_batch\"])\n",
    "    # for epoch in range(GDparams[\"epochs\"]):\n",
    "    while l < GDparams[\"max_cycles\"]:\n",
    "        for j in range(1, runs_in_epoch):\n",
    "            j_start = (j - 1) * GDparams[\"n_batch\"]\n",
    "            j_end = j * GDparams[\"n_batch\"]\n",
    "            \n",
    "            X_batch = X[:, j_start:j_end]\n",
    "            Y_batch = Y[:, j_start:j_end]\n",
    "            \n",
    "            if batch_norm:\n",
    "                grad_W, grad_b, gamma_grad, beta_grad, average_mean, average_variance = compute_gradients(X_batch, Y_batch, layers, lmb, batch_norm, average_mean, average_variance)\n",
    "            else: \n",
    "                grad_W, grad_b = compute_gradients(X_batch, Y_batch, layers, lmb, batch_norm=False)\n",
    "            \n",
    "            for i, layer in enumerate(layers):\n",
    "                layer[\"W\"] = layer[\"W\"] - (eta * grad_W[-1 - i])\n",
    "                layer[\"b\"] = layer[\"b\"] - (eta * grad_b[-1 - i])\n",
    "                \n",
    "                if batch_norm and i != len(layers) - 1:\n",
    "                    layer[\"gamma\"] = layer[\"gamma\"] - (eta * gamma_grad[-1 - i])\n",
    "                    layer[\"beta\"] = layer[\"beta\"] - (eta * beta_grad[-1 - i])\n",
    "                    layer[\"mean\"] = average_mean[i]\n",
    "                    layer[\"var\"] = average_variance[i]\n",
    "        \n",
    "            if calculate_loss and t % 100 == 0:\n",
    "                J_training.append(compute_cost(X, Y, layers, lmb, batch_norm, precomp=False))\n",
    "                J_validation.append(compute_cost(validation[\"X\"], validation[\"Y\"], layers, lmb, batch_norm, precomp=False))\n",
    "                print(\"Step {}, training loss: {}\".format(t, J_training[-1]))\n",
    "                \n",
    "            t += 1 # next update step\n",
    "            if t % (2 * GDparams[\"n_s\"]) == 0:\n",
    "                l += 1 # next cycle\n",
    "                if l == GDparams[\"max_cycles\"]:\n",
    "                    break\n",
    "                print(\"Entering cycle {}, t: {}, eta: {}\".format(l, t, eta))\n",
    "            if t <= (2*l + 1) * GDparams[\"n_s\"]:\n",
    "                eta = GDparams[\"eta_min\"] + (eta_diff * ((t - (2 * l * GDparams[\"n_s\"])) / GDparams[\"n_s\"]))\n",
    "            else:\n",
    "                eta = GDparams[\"eta_max\"] - (eta_diff * (t - ((2*l + 1) * GDparams[\"n_s\"])) / GDparams[\"n_s\"])\n",
    "        \n",
    "        print(\"NEXT EPOCH\")\n",
    "        # shuffle the data after each epoch\n",
    "        shuffled_order = np.random.permutation(X.shape[1])\n",
    "        X = np.take(X, shuffled_order, axis=1)\n",
    "        Y = np.take(Y, shuffled_order, axis=1)\n",
    "\n",
    "    if calculate_loss:\n",
    "        return layers, J_training, J_validation\n",
    "    else:\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_batch': 100, 'eta_min': 1e-05, 'eta_max': 0.1, 'n_s': 2250, 'epochs': 20, 'max_cycles': 2}\n"
     ]
    }
   ],
   "source": [
    "# X, Y, y = load_batch('data/data_batch_1')\n",
    "# X_valid, Y_valid, y_valid = load_batch('data/data_batch_2')\n",
    "X, Y, y, X_valid, Y_valid, y_valid = load_all(validation_size=5000)\n",
    "X_test, Y_test, y_test = load_batch('data/test_batch')\n",
    "\n",
    "d = X.shape[0]\n",
    "N = X.shape[1]\n",
    "K = Y.shape[0]\n",
    "\n",
    "# X: d x N\n",
    "# Y: K x N\n",
    "\n",
    "# m = 50 # hidden units\n",
    "\n",
    "# std_dev_1 = 1 / np.sqrt(d)\n",
    "# std_dev_2 = 1 / np.sqrt(m)\n",
    "# W_1 = std_dev * np.random.randn(m, d)\n",
    "# b_1 = std_dev * np.random.randn(m, 1)\n",
    "\n",
    "# W_2 = std_dev * np.random.randn(K, m)\n",
    "# b_2 = std_dev * np.random.randn(K, 1)\n",
    "def init_network(dimensions=d, num_layers=2, hidden_units=[50], classes=K):\n",
    "    \n",
    "    layers = [\n",
    "        {\n",
    "            \"W\": (1 / np.sqrt(dimensions)) * np.random.randn(hidden_units[0], dimensions),\n",
    "            \"b\": np.zeros((hidden_units[0], 1)),\n",
    "            \"gamma\": np.ones((hidden_units[0], 1)),\n",
    "            \"beta\": np.zeros((hidden_units[0], 1)),\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    if len(hidden_units) > 1:\n",
    "        for prev_i, m in enumerate(hidden_units[1:]):\n",
    "            layers.append({\n",
    "                \"W\": (1 / np.sqrt(hidden_units[prev_i])) * np.random.randn(m, hidden_units[prev_i]),\n",
    "                \"b\": np.zeros((m, 1)),\n",
    "                \"gamma\": np.ones((m, 1)),\n",
    "                \"beta\": np.zeros((m, 1)),\n",
    "            })\n",
    "        \n",
    "    layers.append({\n",
    "            \"W\": (1 / np.sqrt(hidden_units[num_layers-2])) * np.random.randn(classes, hidden_units[num_layers-2]),\n",
    "            \"b\": np.zeros((K, 1)),\n",
    "        })\n",
    "    \n",
    "    return layers\n",
    "\n",
    "\n",
    "# lmb = 0.01  # lambda\n",
    "# lmb = np.power(10, -2.325238114712273)\n",
    "lmb = 0.005\n",
    "GDparams = {\n",
    "    \"n_batch\": 100,\n",
    "    \"eta_min\": 1e-5,\n",
    "    \"eta_max\": 1e-1,\n",
    "    \"n_s\": 2250,\n",
    "    \"epochs\": 20,\n",
    "    \"max_cycles\": 2,\n",
    "}\n",
    "#     \"n_s\": 2 * np.floor(X.shape[1] / 100),\n",
    "validation = {\n",
    "    \"X\": X_valid,\n",
    "    \"Y\": Y_valid,\n",
    "}\n",
    "\n",
    "print(GDparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients_num(X, Y, layers, lmb, h):\n",
    "    \n",
    "    grad_W = [np.zeros(layer[\"W\"].shape) for layer in layers]\n",
    "    grad_b = [np.zeros(layer[\"W\"].shape[0]) for layer in layers]\n",
    "    c = compute_cost(X, Y, layers, lmb)\n",
    "    \n",
    "    for l, layer in enumerate(layers):\n",
    "        for i in range(len(layer[\"b\"])):\n",
    "            layers_try = copy.deepcopy(layers)\n",
    "            layers_try[l][\"b\"][i] = layers_try[l][\"b\"][i] + h\n",
    "            c2 = compute_cost(X, Y, layers_try, lmb)\n",
    "            grad_b[l][i] = (c2 - c) / h\n",
    "\n",
    "        W_shape = layer[\"W\"].shape\n",
    "        for i in range(W_shape[0]):\n",
    "            for j in range(W_shape[1]):\n",
    "                layers_try = copy.deepcopy(layers)\n",
    "                layers_try[l][\"W\"][i,j] = layers_try[l][\"W\"][i,j] + h\n",
    "                c2 = compute_cost(X, Y, layers_try, lmb)\n",
    "                grad_W[l][i,j] = (c2 - c) / h\n",
    "        \n",
    "    return grad_W, grad_b\n",
    "\n",
    "def compute_gradients_num_slow(X, Y, layers, lmb, h, batch_norm=False):\n",
    "    \n",
    "    grad_W = [np.zeros(layer[\"W\"].shape) for layer in layers]\n",
    "    grad_b = [np.zeros(layer[\"b\"].shape[0]) for layer in layers]\n",
    "    grad_gamma = []\n",
    "    grad_beta = []\n",
    "\n",
    "    for l, layer in enumerate(layers):\n",
    "        for i in range(len(layer[\"b\"])):\n",
    "            layers_try1 = copy.deepcopy(layers)\n",
    "            layers_try2 = copy.deepcopy(layers)                                       \n",
    "                                       \n",
    "            layers_try1[l][\"b\"][i] = layers_try1[l][\"b\"][i] - h\n",
    "            c1 = compute_cost(X, Y, layers_try1, lmb, batch_norm)\n",
    "            \n",
    "            \n",
    "            layers_try2[l][\"b\"][i] = layers_try2[l][\"b\"][i] + h\n",
    "            c2 = compute_cost(X, Y, layers_try2, lmb, batch_norm)\n",
    "            \n",
    "            grad_b[l][i] = (c2 - c1) / (2*h)\n",
    "        \n",
    "        W_shape = layer[\"W\"].shape \n",
    "        for i in range(W_shape[0]):\n",
    "            for j in range(W_shape[1]):\n",
    "                layers_try1 = copy.deepcopy(layers)\n",
    "                layers_try2 = copy.deepcopy(layers) \n",
    "\n",
    "                layers_try1[l][\"W\"][i,j] = layers_try1[l][\"W\"][i,j] - h\n",
    "                c1 = compute_cost(X, Y, layers_try1, lmb, batch_norm)\n",
    "                \n",
    "                layers_try2[l][\"W\"][i,j] = layers_try2[l][\"W\"][i,j] + h\n",
    "                c2 = compute_cost(X, Y, layers_try2, lmb, batch_norm)\n",
    "                grad_W[l][i,j] = (c2 - c1) / (2*h)\n",
    "                \n",
    "        if batch_norm and l is not len(layers) - 1:\n",
    "            for i in range(layer[\"gamma\"].shape[0]):\n",
    "                layers_try_gamma1 = copy.deepcopy(layers)\n",
    "                layers_try_gamma2 = copy.deepcopy(layers)\n",
    "                \n",
    "                layers_try_gamma1[l][\"gamma\"][i] = layers_try_gamma1[l][\"gamma\"][i] - h\n",
    "                c1 = compute_cost(X, Y, layers_try_gamma1, lmb, batch_norm)\n",
    "\n",
    "                layers_try_gamma2[l][\"gamma\"][i] = layers_try_gamma2[l][\"gamma\"][i] + h\n",
    "                c2 = compute_cost(X, Y, layers_try_gamma2, lmb, batch_norm)\n",
    "\n",
    "                grad_gamma.append((c2 - c1) / (2*h))\n",
    "                \n",
    "            for i in range(layer[\"beta\"].shape[0]):                \n",
    "                layers_try_beta1 = copy.deepcopy(layers)\n",
    "                layers_try_beta2 = copy.deepcopy(layers)\n",
    "\n",
    "                layers_try_beta1[l][\"beta\"][i] = layers_try_beta1[l][\"beta\"][i] - h\n",
    "                c1 = compute_cost(X, Y, layers_try_beta1, lmb, batch_norm)\n",
    "\n",
    "                layers_try_beta2[l][\"beta\"][i] = layers_try_beta2[l][\"beta\"][i] + h\n",
    "                c2 = compute_cost(X, Y, layers_try_beta2, lmb, batch_norm)\n",
    "\n",
    "                grad_beta.append((c2 - c1) / (2*h))\n",
    "            \n",
    "    \n",
    "    return grad_W, grad_b, grad_gamma, grad_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1)\n",
      "(50, 1)\n",
      "\n",
      "grad b (last to first):\n",
      "Layer 2 mean diff: 4.4415337380199205e-06\n",
      "Layer 1 mean diff: 5.939191245298004e-10\n",
      "\n",
      "grad W (last to first):\n",
      "Layer 2 mean diff: 0.0001244175541816699\n",
      "Layer 1 mean diff: 1.4062122794891166e-09\n",
      "\n",
      "grad gamma (last to first):\n",
      "[[ 5.57934219e-02]\n",
      " [-1.29358133e-02]\n",
      " [ 1.43291626e-01]\n",
      " [-1.01760155e-01]\n",
      " [ 9.53168627e-02]\n",
      " [ 8.41427014e-03]\n",
      " [ 2.35437675e-02]\n",
      " [ 1.49132939e-01]\n",
      " [-9.13131664e-03]\n",
      " [-1.26705228e-02]\n",
      " [-9.46918551e-02]\n",
      " [-8.20661553e-02]\n",
      " [-1.11485762e-02]\n",
      " [ 5.80210361e-02]\n",
      " [ 3.28562217e-02]\n",
      " [ 5.15663765e-03]\n",
      " [ 1.37919205e-01]\n",
      " [-1.37193385e-04]\n",
      " [ 1.52086905e-01]\n",
      " [ 6.24893599e-02]\n",
      " [ 4.79028003e-02]\n",
      " [ 1.23719306e-01]\n",
      " [-7.47050509e-03]\n",
      " [ 3.11202225e-02]\n",
      " [ 7.08140152e-02]\n",
      " [ 2.98453996e-02]\n",
      " [ 5.58220039e-02]\n",
      " [ 3.56437429e-02]\n",
      " [-4.24276471e-02]\n",
      " [-4.33454809e-02]\n",
      " [ 7.30562013e-02]\n",
      " [ 6.67411670e-02]\n",
      " [-3.15647925e-02]\n",
      " [-3.86977917e-02]\n",
      " [ 7.02650538e-02]\n",
      " [-5.10252055e-02]\n",
      " [-4.86599253e-02]\n",
      " [ 1.57259847e-01]\n",
      " [ 1.41474166e-01]\n",
      " [ 5.82151830e-02]\n",
      " [ 4.48991554e-02]\n",
      " [ 2.54034751e-03]\n",
      " [ 1.09076499e-01]\n",
      " [ 2.00386100e-02]\n",
      " [-9.52351397e-03]\n",
      " [-1.67408760e-02]\n",
      " [-5.11117892e-02]\n",
      " [ 7.06601540e-03]\n",
      " [-5.80009941e-04]\n",
      " [-1.32229420e-01]]\n",
      "0.05579342166051049\n",
      "Layer 1 mean diff: 0.5919068600354486\n",
      "\n",
      "grad beta (last to first):\n",
      "Layer 1 mean diff: 0.5919068978487284\n"
     ]
    }
   ],
   "source": [
    "feature_dims = 10\n",
    "samples = 2\n",
    "\n",
    "layers_g = init_network(dimensions=feature_dims, num_layers=2, hidden_units=[50], classes=K)\n",
    "X_g = X[0:feature_dims, 0:samples] #d X N\n",
    "Y_g = Y[:, 0:samples] #K x N\n",
    "\n",
    "grad_W, grad_b, grad_gamma, grad_beta = compute_gradients(X_g, Y_g, layers_g, lmb=0, batch_norm=True)[:4]\n",
    "ngrad_w, ngrad_b, ngrad_gamma, ngrad_beta = compute_gradients_num_slow(X_g, Y_g, layers_g, lmb=0, h=1e-6, batch_norm=True)\n",
    "print(\"\\ngrad b (last to first):\")\n",
    "\n",
    "\n",
    "for i in range(len(grad_b)):\n",
    "    diffs = np.abs(grad_b[len(grad_b) - 1 - i].T - ngrad_b[i]) / np.maximum(1e-6, np.abs(grad_b[len(grad_b) - 1 - i].T) + np.abs(ngrad_b[i]))\n",
    "#     print(diffs)\n",
    "    print(\"Layer {} mean diff: {}\".format(len(grad_b) - i, np.mean(diffs)))\n",
    "\n",
    "\n",
    "print(\"\\ngrad W (last to first):\")\n",
    "for i in range(len(grad_W)):\n",
    "    diffs = np.abs(grad_W[len(grad_W) - 1 - i] - ngrad_w[i]) / np.maximum(1e-6, np.abs(grad_W[len(grad_W) - 1 - i]) + np.abs(ngrad_w[i]))\n",
    "#     print(diffs)\n",
    "    print(\"Layer {} mean diff: {}\".format(len(grad_W) - i, np.mean(diffs)))\n",
    "    \n",
    "print(\"\\ngrad gamma (last to first):\")\n",
    "for i in range(len(grad_gamma)):\n",
    "    print(grad_gamma[i])\n",
    "    print(ngrad_gamma[i])\n",
    "#     print(np.abs(grad_gamma[len(grad_gamma) - 1 - i] - ngrad_gamma[i]))\n",
    "    diffs = np.abs(grad_gamma[len(grad_gamma) - 1 - i] - ngrad_gamma[i]) / np.maximum(1e-6, np.abs(grad_gamma[len(grad_gamma) - 1 - i]) + np.abs(ngrad_gamma[i]))\n",
    "#     print(diffs)\n",
    "    print(\"Layer {} mean diff: {}\".format(len(grad_gamma) - i, np.mean(diffs)))\n",
    "    \n",
    "print(\"\\ngrad beta (last to first):\")\n",
    "for i in range(len(grad_beta)):\n",
    "    diffs = np.abs(grad_beta[len(grad_beta) - 1 - i] - ngrad_beta[i]) / np.maximum(1e-6, np.abs(grad_beta[len(grad_beta) - 1 - i]) + np.abs(ngrad_beta[i]))\n",
    "#     print(diffs)\n",
    "    print(\"Layer {} mean diff: {}\".format(len(grad_beta) - i, np.mean(diffs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, training loss: 3.0542220026379714\n",
      "Step 100, training loss: 2.78145366654734\n",
      "Step 200, training loss: 2.5451962796101353\n",
      "Step 300, training loss: 2.3759652804678355\n",
      "Step 400, training loss: 2.2757495780527375\n",
      "NEXT EPOCH\n",
      "Step 500, training loss: 2.207150353337256\n",
      "Step 600, training loss: 2.1376165428219625\n",
      "Step 700, training loss: 2.088212582559587\n",
      "Step 800, training loss: 2.032389720513683\n",
      "NEXT EPOCH\n",
      "Step 900, training loss: 1.9769404255721215\n",
      "Step 1000, training loss: 1.935790832459436\n",
      "Step 1100, training loss: 1.9117730856625768\n",
      "Step 1200, training loss: 1.8630613147063986\n",
      "Step 1300, training loss: 1.834305742731938\n",
      "NEXT EPOCH\n",
      "Step 1400, training loss: 1.838208935484769\n",
      "Step 1500, training loss: 1.7766065564805396\n",
      "Step 1600, training loss: 1.7445709195067787\n",
      "Step 1700, training loss: 1.7179420727662922\n",
      "NEXT EPOCH\n",
      "Step 1800, training loss: 1.727649062942914\n",
      "Step 1900, training loss: 1.7439609853975477\n",
      "Step 2000, training loss: 1.6430130474969977\n",
      "Step 2100, training loss: 1.6398239947501\n",
      "Step 2200, training loss: 1.6861667441722095\n",
      "NEXT EPOCH\n",
      "Step 2300, training loss: 1.6924609201260017\n",
      "Step 2400, training loss: 1.6319458622276803\n",
      "Step 2500, training loss: 1.6580107715635997\n",
      "Step 2600, training loss: 1.6174647426377688\n",
      "NEXT EPOCH\n",
      "Step 2700, training loss: 1.599839295895001\n",
      "Step 2800, training loss: 1.5548347211911078\n",
      "Step 2900, training loss: 1.5571866763965385\n",
      "Step 3000, training loss: 1.5434956390630117\n",
      "Step 3100, training loss: 1.5368664100695566\n",
      "NEXT EPOCH\n",
      "Step 3200, training loss: 1.5271621335552636\n",
      "Step 3300, training loss: 1.504315790244967\n",
      "Step 3400, training loss: 1.5081471168971112\n",
      "Step 3500, training loss: 1.4941219257176501\n",
      "NEXT EPOCH\n",
      "Step 3600, training loss: 1.4940564198894375\n",
      "Step 3700, training loss: 1.47401166055148\n",
      "Step 3800, training loss: 1.4817986589147227\n",
      "Step 3900, training loss: 1.4618034543332263\n",
      "Step 4000, training loss: 1.4526482692321359\n",
      "NEXT EPOCH\n",
      "Step 4100, training loss: 1.4458984286314434\n",
      "Step 4200, training loss: 1.4378784129303166\n",
      "Step 4300, training loss: 1.4291175572579333\n",
      "Step 4400, training loss: 1.4220188889369458\n",
      "NEXT EPOCH\n",
      "Entering cycle 1, t: 4500, eta: 5.443999999998894e-05\n",
      "Step 4500, training loss: 1.4193841359398118\n",
      "Step 4600, training loss: 1.421353262523953\n",
      "Step 4700, training loss: 1.4252287695404704\n",
      "Step 4800, training loss: 1.4309779927596251\n",
      "Step 4900, training loss: 1.4298282126315711\n",
      "NEXT EPOCH\n",
      "Step 5000, training loss: 1.4408967244055837\n",
      "Step 5100, training loss: 1.4450656065717407\n",
      "Step 5200, training loss: 1.4724257010603645\n",
      "Step 5300, training loss: 1.46241754475094\n",
      "NEXT EPOCH\n",
      "Step 5400, training loss: 1.4559686621728036\n",
      "Step 5500, training loss: 1.4832636859526238\n",
      "Step 5600, training loss: 1.5385509839653915\n",
      "Step 5700, training loss: 1.5090186301477648\n",
      "Step 5800, training loss: 1.494506938299344\n",
      "NEXT EPOCH\n",
      "Step 5900, training loss: 1.5320456061158187\n",
      "Step 6000, training loss: 1.5002005671715137\n",
      "Step 6100, training loss: 1.533577877898435\n",
      "Step 6200, training loss: 1.5444810828349038\n",
      "NEXT EPOCH\n",
      "Step 6300, training loss: 1.5352408284057435\n",
      "Step 6400, training loss: 1.5748242200461617\n",
      "Step 6500, training loss: 1.5915523651417274\n",
      "Step 6600, training loss: 1.5560085597680602\n",
      "Step 6700, training loss: 1.5769668073792777\n",
      "NEXT EPOCH\n",
      "Step 6800, training loss: 1.5658001105194426\n",
      "Step 6900, training loss: 1.5833840671747177\n",
      "Step 7000, training loss: 1.577373420187471\n",
      "Step 7100, training loss: 1.5487804987344855\n",
      "NEXT EPOCH\n",
      "Step 7200, training loss: 1.5260880666039431\n",
      "Step 7300, training loss: 1.5340220861448723\n",
      "Step 7400, training loss: 1.5501796412075175\n",
      "Step 7500, training loss: 1.5084943109863662\n",
      "Step 7600, training loss: 1.4838506075243385\n",
      "NEXT EPOCH\n",
      "Step 7700, training loss: 1.5210238608178086\n",
      "Step 7800, training loss: 1.4656648850325187\n",
      "Step 7900, training loss: 1.4730950109361656\n",
      "Step 8000, training loss: 1.4627024633239711\n",
      "NEXT EPOCH\n",
      "Step 8100, training loss: 1.449131308189445\n",
      "Step 8200, training loss: 1.4556815904531524\n",
      "Step 8300, training loss: 1.4606781962161992\n",
      "Step 8400, training loss: 1.4260207829963876\n",
      "Step 8500, training loss: 1.4112199547190982\n",
      "NEXT EPOCH\n",
      "Step 8600, training loss: 1.409824369838128\n",
      "Step 8700, training loss: 1.3993097304564703\n",
      "Step 8800, training loss: 1.3931937146702227\n",
      "Step 8900, training loss: 1.3896919242603203\n",
      "NEXT EPOCH\n",
      "NEXT EPOCH\n"
     ]
    }
   ],
   "source": [
    "# 3-layers\n",
    "layers = init_network(num_layers=3, hidden_units=[50,50])\n",
    "layers_trained, J_training, J_validation = mini_batch_GD(X, Y, GDparams, layers, lmb, validation, calculate_loss=True)\n",
    "test_cost = compute_cost(X_test, Y_test, layers_trained, lmb)\n",
    "test_acc = compute_accuracy(X_test, y_test, layers_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cost:  1.5317799786685302\n",
      "Test accuracy:  0.5328\n"
     ]
    }
   ],
   "source": [
    "plt.figure(0)\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.plot([step*100 for step in range(len(J_training))], J_training, label=\"Training\")\n",
    "plt.plot([step*100 for step in range(len(J_validation))], J_validation, label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Test cost: \", test_cost)\n",
    "print(\"Test accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, training loss: 3.1584437575891635\n",
      "Step 100, training loss: 3.1520614680715346\n",
      "Step 200, training loss: 3.135819936693884\n",
      "Step 300, training loss: 3.1078112771470674\n",
      "Step 400, training loss: 3.0513730506310366\n",
      "NEXT EPOCH\n",
      "Step 500, training loss: 2.9738315667729203\n",
      "Step 600, training loss: 2.8947909573295187\n",
      "Step 700, training loss: 2.815551135262776\n",
      "Step 800, training loss: 2.7375363189368795\n",
      "NEXT EPOCH\n",
      "Step 900, training loss: 2.6682975980317423\n",
      "Step 1000, training loss: 2.597804857393734\n",
      "Step 1100, training loss: 2.528872282762256\n",
      "Step 1200, training loss: 2.4643585054350123\n",
      "Step 1300, training loss: 2.407973198316094\n",
      "NEXT EPOCH\n",
      "Step 1400, training loss: 2.3704668797249475\n",
      "Step 1500, training loss: 2.34997800833626\n",
      "Step 1600, training loss: 2.3062409295931334\n",
      "Step 1700, training loss: 2.2353427434335025\n",
      "NEXT EPOCH\n",
      "Step 1800, training loss: 2.208537533399923\n",
      "Step 1900, training loss: 2.1616591000697443\n",
      "Step 2000, training loss: 2.143648523774099\n",
      "Step 2100, training loss: 2.1324316807997796\n",
      "Step 2200, training loss: 2.1338303414141957\n",
      "NEXT EPOCH\n",
      "Step 2300, training loss: 2.1272254513962863\n",
      "Step 2400, training loss: 2.0784327024774045\n",
      "Step 2500, training loss: 2.0709512935895527\n",
      "Step 2600, training loss: 2.072854332864722\n",
      "NEXT EPOCH\n",
      "Step 2700, training loss: 2.0424936590324854\n",
      "Step 2800, training loss: 2.0280166882314785\n",
      "Step 2900, training loss: 2.015130448530239\n",
      "Step 3000, training loss: 2.0085802687711913\n",
      "Step 3100, training loss: 1.9972387697001117\n",
      "NEXT EPOCH\n",
      "Step 3200, training loss: 1.981476279595104\n",
      "Step 3300, training loss: 1.97772511437406\n",
      "Step 3400, training loss: 1.9718601860303928\n",
      "Step 3500, training loss: 1.9925037384383872\n",
      "NEXT EPOCH\n",
      "Step 3600, training loss: 1.9558947080159694\n",
      "Step 3700, training loss: 1.9623022520107105\n",
      "Step 3800, training loss: 1.9357589691608197\n",
      "Step 3900, training loss: 1.9403365066542226\n",
      "Step 4000, training loss: 1.933525250418285\n",
      "NEXT EPOCH\n",
      "Step 4100, training loss: 1.9288669474500384\n",
      "Step 4200, training loss: 1.9151444927697792\n",
      "Step 4300, training loss: 1.9051307225985672\n",
      "Step 4400, training loss: 1.8997187412729721\n",
      "NEXT EPOCH\n",
      "Entering cycle 1, t: 4500, eta: 5.443999999998894e-05\n",
      "Step 4500, training loss: 1.8967586897352389\n",
      "Step 4600, training loss: 1.896157187323865\n",
      "Step 4700, training loss: 1.903203835598283\n",
      "Step 4800, training loss: 1.9059737307939224\n",
      "Step 4900, training loss: 1.9085420500165025\n",
      "NEXT EPOCH\n",
      "Step 5000, training loss: 1.9054566608497179\n",
      "Step 5100, training loss: 1.9153770569825483\n",
      "Step 5200, training loss: 1.9266258279830515\n",
      "Step 5300, training loss: 1.9205201603748792\n",
      "NEXT EPOCH\n",
      "Step 5400, training loss: 1.9400017930462878\n",
      "Step 5500, training loss: 1.9423618195954009\n",
      "Step 5600, training loss: 1.9393836611814577\n",
      "Step 5700, training loss: 1.9499504993215004\n",
      "Step 5800, training loss: 1.9844283947529482\n",
      "NEXT EPOCH\n",
      "Step 5900, training loss: 1.9867002256501316\n",
      "Step 6000, training loss: 1.9632732039496996\n",
      "Step 6100, training loss: 2.0042184657813675\n",
      "Step 6200, training loss: 1.979970450907405\n",
      "NEXT EPOCH\n",
      "Step 6300, training loss: 1.995508930818513\n",
      "Step 6400, training loss: 1.9556576384710138\n",
      "Step 6500, training loss: 1.9694015352542942\n",
      "Step 6600, training loss: 2.0001196470788534\n",
      "Step 6700, training loss: 1.9922557504155494\n",
      "NEXT EPOCH\n",
      "Step 6800, training loss: 2.005404750505383\n",
      "Step 6900, training loss: 1.9785157328435017\n",
      "Step 7000, training loss: 1.960328658033537\n",
      "Step 7100, training loss: 1.95134879422851\n",
      "NEXT EPOCH\n",
      "Step 7200, training loss: 1.9448473006606524\n",
      "Step 7300, training loss: 1.9563850732577985\n",
      "Step 7400, training loss: 1.952063617160767\n",
      "Step 7500, training loss: 1.9436829568576712\n",
      "Step 7600, training loss: 1.9477752949002358\n",
      "NEXT EPOCH\n",
      "Step 7700, training loss: 1.9271124278233733\n",
      "Step 7800, training loss: 1.927430231892639\n",
      "Step 7900, training loss: 1.9425539222855324\n",
      "Step 8000, training loss: 1.9205300490098198\n",
      "NEXT EPOCH\n",
      "Step 8100, training loss: 1.8983009313061463\n",
      "Step 8200, training loss: 1.8926945936979096\n",
      "Step 8300, training loss: 1.8862318202125778\n",
      "Step 8400, training loss: 1.8717906929535564\n",
      "Step 8500, training loss: 1.86811151636579\n",
      "NEXT EPOCH\n",
      "Step 8600, training loss: 1.8659997695281063\n",
      "Step 8700, training loss: 1.8513214641959763\n",
      "Step 8800, training loss: 1.8431580018164475\n",
      "Step 8900, training loss: 1.8326110290045814\n",
      "NEXT EPOCH\n",
      "NEXT EPOCH\n"
     ]
    }
   ],
   "source": [
    "layers9 = init_network(num_layers=9, hidden_units=[50,30,20,20,10,10,10,10])\n",
    "layers9_trained, J_training9, J_validation9 = mini_batch_GD(X, Y, GDparams, layers9, lmb, validation, calculate_loss=True)\n",
    "test_cost9 = compute_cost(X_test, Y_test, layers9_trained, lmb)\n",
    "test_acc9 = compute_accuracy(X_test, y_test, layers9_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cost:  1.921850470281229\n",
      "Test accuracy:  0.3276\n"
     ]
    }
   ],
   "source": [
    "plt.figure(0)\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.plot([step*100 for step in range(len(J_training9))], J_training9, label=\"Training\")\n",
    "plt.plot([step*100 for step in range(len(J_validation9))], J_validation9, label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Test cost: \", test_cost9)\n",
    "print(\"Test accuracy: \", test_acc9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer shapes:\n",
      "(50, 3072)\n",
      "(10, 50)\n",
      "---\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "Step 0, training loss: 2.819795792204788\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "Step 100, training loss: 2.347325369740482\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n",
      "(10, 50)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-e6fe65ba1751>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"---\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mlayers_trained\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mJ_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mJ_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmini_batch_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGDparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlmb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtest_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_trained\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlmb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_trained\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-83-f667cd2a62e7>\u001b[0m in \u001b[0;36mmini_batch_GD\u001b[1;34m(X, Y, GDparams, layers, lmb, validation, batch_norm, calculate_loss)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcalculate_loss\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0mJ_training\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompute_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlmb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[0mJ_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompute_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Y\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlmb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Step {}, training loss: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mJ_training\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-89-29dc8113b611>\u001b[0m in \u001b[0;36mcompute_cost\u001b[1;34m(X, Y, layers, lmb)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlmb\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mw_square_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcross_entropy\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlmb\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw_square_sum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2-layers NON BN\n",
    "layers = init_network(num_layers=2, hidden_units=[50])\n",
    "print(\"Layer shapes:\")\n",
    "for l in layers:\n",
    "    print(l[\"W\"].shape)\n",
    "print(\"---\")\n",
    "layers_trained, J_training, J_validation = mini_batch_GD(X, Y, GDparams, layers, lmb, validation, batch_norm=False, calculate_loss=True)\n",
    "test_cost = compute_cost(X_test, Y_test, layers_trained, lmb)\n",
    "test_acc = compute_accuracy(X_test, y_test, layers_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer shapes:\n",
      "(50, 3072)\n",
      "(30, 50)\n",
      "(20, 30)\n",
      "(20, 20)\n",
      "(10, 20)\n",
      "(10, 10)\n",
      "(10, 10)\n",
      "(10, 10)\n",
      "(10, 10)\n",
      "---\n",
      "Step 0, training loss: 3.4108786575080767\n",
      "Step 100, training loss: 3.2819138709793156\n",
      "Step 200, training loss: 3.172850335253418\n",
      "Step 300, training loss: 3.0311697457955322\n",
      "Step 400, training loss: 2.936101175313134\n",
      "NEXT EPOCH\n",
      "Step 500, training loss: 2.8494701610137785\n",
      "Step 600, training loss: 2.768144329592506\n",
      "Step 700, training loss: 2.687285562412945\n",
      "Step 800, training loss: 2.6138018013165567\n",
      "NEXT EPOCH\n",
      "Step 900, training loss: 2.532482768636704\n",
      "Step 1000, training loss: 2.4573556343431138\n",
      "Step 1100, training loss: 2.378694345533178\n",
      "Step 1200, training loss: 2.3027683359119524\n",
      "Step 1300, training loss: 2.232687054015529\n",
      "NEXT EPOCH\n",
      "Step 1400, training loss: 2.163947268988898\n",
      "Step 1500, training loss: 2.1215493181216214\n",
      "Step 1600, training loss: 2.049600149832533\n",
      "Step 1700, training loss: 2.036013704412895\n",
      "NEXT EPOCH\n",
      "Step 1800, training loss: 1.9677067781183066\n",
      "Step 1900, training loss: 1.9556466185230286\n",
      "Step 2000, training loss: 1.9070371534621386\n",
      "Step 2100, training loss: 1.8788165477771948\n",
      "Step 2200, training loss: 1.8885754698902273\n",
      "NEXT EPOCH\n",
      "Step 2300, training loss: 1.8633643040268097\n",
      "Step 2400, training loss: 1.8266963580813238\n",
      "Step 2500, training loss: 1.8328801426813817\n",
      "Step 2600, training loss: 1.8166502384833294\n",
      "NEXT EPOCH\n",
      "Step 2700, training loss: 1.7681772393787942\n",
      "Step 2800, training loss: 1.7665185498541398\n",
      "Step 2900, training loss: 1.7610153894624074\n",
      "Step 3000, training loss: 1.7787148051338018\n",
      "Step 3100, training loss: 1.7200010856867836\n",
      "NEXT EPOCH\n",
      "Step 3200, training loss: 1.7198744362596925\n",
      "Step 3300, training loss: 1.706505851134789\n",
      "Step 3400, training loss: 1.7127123694800284\n",
      "Step 3500, training loss: 1.6964461874874786\n",
      "NEXT EPOCH\n",
      "Step 3600, training loss: 1.6724300662870992\n",
      "Step 3700, training loss: 1.6508177858293378\n",
      "Step 3800, training loss: 1.6458704189910587\n",
      "Step 3900, training loss: 1.6157451357582957\n",
      "Step 4000, training loss: 1.6001347405010915\n",
      "NEXT EPOCH\n",
      "Step 4100, training loss: 1.5793891853929805\n",
      "Step 4200, training loss: 1.5743351629028224\n",
      "Step 4300, training loss: 1.5525578366767159\n",
      "Step 4400, training loss: 1.53503776000165\n",
      "NEXT EPOCH\n",
      "Entering cycle 1, t: 4500, eta: 5.443999999998894e-05\n",
      "Step 4500, training loss: 1.5276797161855895\n",
      "Step 4600, training loss: 1.5266366700159955\n",
      "Step 4700, training loss: 1.5348168791745584\n",
      "Step 4800, training loss: 1.5483123507930672\n",
      "Step 4900, training loss: 1.56164368414429\n",
      "NEXT EPOCH\n",
      "Step 5000, training loss: 1.565803779628857\n",
      "Step 5100, training loss: 1.601153489053624\n",
      "Step 5200, training loss: 1.6055263385563625\n",
      "Step 5300, training loss: 1.6299470422204299\n",
      "NEXT EPOCH\n",
      "Step 5400, training loss: 1.6444390698693545\n",
      "Step 5500, training loss: 1.724149925445818\n",
      "Step 5600, training loss: 1.7451219929809347\n",
      "Step 5700, training loss: 1.7013774206094485\n",
      "Step 5800, training loss: 1.6954673172275385\n",
      "NEXT EPOCH\n",
      "Step 5900, training loss: 1.682365308385093\n",
      "Step 6000, training loss: 1.7102249142567363\n",
      "Step 6100, training loss: 1.7494532359986437\n",
      "Step 6200, training loss: 1.7469790518837394\n",
      "NEXT EPOCH\n",
      "Step 6300, training loss: 1.7505884051161882\n",
      "Step 6400, training loss: 1.7604244911599076\n",
      "Step 6500, training loss: 1.7640770623051587\n",
      "Step 6600, training loss: 1.8129019023883517\n",
      "Step 6700, training loss: 1.7619479939150489\n",
      "NEXT EPOCH\n",
      "Step 6800, training loss: 1.8072691064011235\n",
      "Step 6900, training loss: 1.8085259952359183\n",
      "Step 7000, training loss: 1.7871904819199917\n",
      "Step 7100, training loss: 1.7456114621021617\n",
      "NEXT EPOCH\n",
      "Step 7200, training loss: 1.761907847115437\n",
      "Step 7300, training loss: 1.7472323874631328\n",
      "Step 7400, training loss: 1.7371172576175904\n",
      "Step 7500, training loss: 1.723041758282306\n",
      "Step 7600, training loss: 1.7156941918512323\n",
      "NEXT EPOCH\n",
      "Step 7700, training loss: 1.716576470394708\n",
      "Step 7800, training loss: 1.6925976948043528\n",
      "Step 7900, training loss: 1.7031324357964874\n",
      "Step 8000, training loss: 1.6717797192495676\n",
      "NEXT EPOCH\n",
      "Step 8100, training loss: 1.6665802276930763\n",
      "Step 8200, training loss: 1.6392554930818877\n",
      "Step 8300, training loss: 1.6303075053765257\n",
      "Step 8400, training loss: 1.6143601143882398\n",
      "Step 8500, training loss: 1.5914035286156103\n",
      "NEXT EPOCH\n",
      "Step 8600, training loss: 1.560430425116436\n",
      "Step 8700, training loss: 1.5503872607145983\n",
      "Step 8800, training loss: 1.5332704982053982\n",
      "Step 8900, training loss: 1.5161626957846015\n",
      "NEXT EPOCH\n",
      "NEXT EPOCH\n",
      "USING PRECOMPUTED MEANS\n",
      "[ 0.03350724 -0.04669177 -0.03941984  0.00039891 -0.08111301  0.00742704\n",
      "  0.02789164 -0.0147464  -0.01320149 -0.03886985 -0.04015374 -0.01422361\n",
      " -0.00227929  0.01128589  0.00517759  0.05480726 -0.03216303 -0.04139984\n",
      " -0.01935718 -0.07891399  0.00381118  0.00026398  0.02413032  0.04472114\n",
      "  0.03625925 -0.02939789  0.00420729  0.01441036  0.00757368 -0.1412338\n",
      "  0.00752369 -0.03333832 -0.01689542 -0.06281898  0.02745966 -0.08799013\n",
      " -0.10520264  0.06585045 -0.02076309 -0.00631473 -0.00278303 -0.13448512\n",
      "  0.02274208  0.03750473 -0.00520593  0.0719959  -0.07265552  0.01186466\n",
      "  0.01778553  0.05756394]\n",
      "USING PRECOMPUTED MEANS\n",
      "[-0.27142847 -0.31303558  0.07571825 -0.05535453  0.39159988 -0.19425183\n",
      " -0.12754253 -0.03039103 -0.18266214 -0.33685789 -0.07225464 -0.10960371\n",
      " -0.43248548 -0.11856471 -0.08530237 -0.17604033  0.01294539 -0.04594146\n",
      " -0.49940515 -0.3760209  -0.29151428 -0.23691042 -0.08024834 -0.15763375\n",
      " -0.20699002 -0.36326522 -0.390712    0.03592889  0.00198485  0.03525337]\n",
      "USING PRECOMPUTED MEANS\n",
      "[ 0.03701514 -0.02610093 -0.12927907  0.00187531 -0.2195616  -0.03546431\n",
      " -0.1978589  -0.13892587  0.01044552 -0.12775202 -0.21703643 -0.14999878\n",
      " -0.09979474 -0.17855927 -0.15815957 -0.17385663 -0.07335694 -0.08616493\n",
      " -0.06366345 -0.10408853]\n",
      "USING PRECOMPUTED MEANS\n",
      "[-0.04175038 -0.04740765 -0.05708641 -0.03516778 -0.03796688 -0.0368357\n",
      " -0.04561374 -0.05595734 -0.07053629 -0.05624363 -0.03971141 -0.05449271\n",
      " -0.01343633 -0.04094721 -0.05812223 -0.02272869 -0.05255822 -0.01995905\n",
      " -0.086642   -0.03566784]\n",
      "USING PRECOMPUTED MEANS\n",
      "[-0.17981721 -0.18979323 -0.17531529 -0.11715025 -0.20200196 -0.23671117\n",
      " -0.16777207 -0.21857481 -0.24375528 -0.33142672]\n",
      "USING PRECOMPUTED MEANS\n",
      "[-0.05367542 -0.06634585 -0.06616655 -0.0688011  -0.03047778 -0.06403805\n",
      " -0.04264617 -0.06146584 -0.06079011 -0.06414264]\n",
      "USING PRECOMPUTED MEANS\n",
      "[-0.03747848 -0.08483509 -0.03093793 -0.07037573 -0.05474649 -0.08914056\n",
      " -0.02589905 -0.05338985 -0.09745648 -0.02452052]\n",
      "USING PRECOMPUTED MEANS\n",
      "[0.07667415 0.09313541 0.04645932 0.03959142 0.05188136 0.0107985\n",
      " 0.06447375 0.04526001 0.05480929 0.08226528]\n",
      "USING PRECOMPUTED MEANS\n",
      "[ 0.03350724 -0.04669177 -0.03941984  0.00039891 -0.08111301  0.00742704\n",
      "  0.02789164 -0.0147464  -0.01320149 -0.03886985 -0.04015374 -0.01422361\n",
      " -0.00227929  0.01128589  0.00517759  0.05480726 -0.03216303 -0.04139984\n",
      " -0.01935718 -0.07891399  0.00381118  0.00026398  0.02413032  0.04472114\n",
      "  0.03625925 -0.02939789  0.00420729  0.01441036  0.00757368 -0.1412338\n",
      "  0.00752369 -0.03333832 -0.01689542 -0.06281898  0.02745966 -0.08799013\n",
      " -0.10520264  0.06585045 -0.02076309 -0.00631473 -0.00278303 -0.13448512\n",
      "  0.02274208  0.03750473 -0.00520593  0.0719959  -0.07265552  0.01186466\n",
      "  0.01778553  0.05756394]\n",
      "USING PRECOMPUTED MEANS\n",
      "[-0.27142847 -0.31303558  0.07571825 -0.05535453  0.39159988 -0.19425183\n",
      " -0.12754253 -0.03039103 -0.18266214 -0.33685789 -0.07225464 -0.10960371\n",
      " -0.43248548 -0.11856471 -0.08530237 -0.17604033  0.01294539 -0.04594146\n",
      " -0.49940515 -0.3760209  -0.29151428 -0.23691042 -0.08024834 -0.15763375\n",
      " -0.20699002 -0.36326522 -0.390712    0.03592889  0.00198485  0.03525337]\n",
      "USING PRECOMPUTED MEANS\n",
      "[ 0.03701514 -0.02610093 -0.12927907  0.00187531 -0.2195616  -0.03546431\n",
      " -0.1978589  -0.13892587  0.01044552 -0.12775202 -0.21703643 -0.14999878\n",
      " -0.09979474 -0.17855927 -0.15815957 -0.17385663 -0.07335694 -0.08616493\n",
      " -0.06366345 -0.10408853]\n",
      "USING PRECOMPUTED MEANS\n",
      "[-0.04175038 -0.04740765 -0.05708641 -0.03516778 -0.03796688 -0.0368357\n",
      " -0.04561374 -0.05595734 -0.07053629 -0.05624363 -0.03971141 -0.05449271\n",
      " -0.01343633 -0.04094721 -0.05812223 -0.02272869 -0.05255822 -0.01995905\n",
      " -0.086642   -0.03566784]\n",
      "USING PRECOMPUTED MEANS\n",
      "[-0.17981721 -0.18979323 -0.17531529 -0.11715025 -0.20200196 -0.23671117\n",
      " -0.16777207 -0.21857481 -0.24375528 -0.33142672]\n",
      "USING PRECOMPUTED MEANS\n",
      "[-0.05367542 -0.06634585 -0.06616655 -0.0688011  -0.03047778 -0.06403805\n",
      " -0.04264617 -0.06146584 -0.06079011 -0.06414264]\n",
      "USING PRECOMPUTED MEANS\n",
      "[-0.03747848 -0.08483509 -0.03093793 -0.07037573 -0.05474649 -0.08914056\n",
      " -0.02589905 -0.05338985 -0.09745648 -0.02452052]\n",
      "USING PRECOMPUTED MEANS\n",
      "[0.07667415 0.09313541 0.04645932 0.03959142 0.05188136 0.0107985\n",
      " 0.06447375 0.04526001 0.05480929 0.08226528]\n"
     ]
    }
   ],
   "source": [
    "# 2-layers\n",
    "layers = init_network(num_layers=9, hidden_units=[50,30,20,20,10,10,10,10])\n",
    "print(\"Layer shapes:\")\n",
    "for l in layers:\n",
    "    print(l[\"W\"].shape)\n",
    "print(\"---\")\n",
    "layers_trained, J_training, J_validation = mini_batch_GD(X, Y, GDparams, layers, lmb, validation, batch_norm=True, calculate_loss=True)\n",
    "\n",
    "\n",
    "test_cost = compute_cost(X_test, Y_test, layers_trained, lmb, batch_norm=True, precomp=True)\n",
    "test_acc = compute_accuracy(X_test, y_test, layers_trained, batch_norm=True, precomp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cost:  1.6235918723568925\n",
      "Test accuracy:  0.4991\n"
     ]
    }
   ],
   "source": [
    "plt.figure(0)\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.plot([step*100 for step in range(len(J_training))], J_training, label=\"Training\")\n",
    "plt.plot([step*100 for step in range(len(J_validation))], J_validation, label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Test cost: \", test_cost)\n",
    "print(\"Test accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W': array([[-0.01538028,  0.03797142, -0.05959421, ...,  0.0404259 ,\n",
      "         0.03262774, -0.03571147],\n",
      "       [ 0.01921223,  0.0408635 , -0.01719921, ..., -0.08768962,\n",
      "         0.02070206, -0.02417298],\n",
      "       [ 0.00243656, -0.06825153,  0.01091312, ..., -0.01290415,\n",
      "         0.0425736 , -0.01299054],\n",
      "       ...,\n",
      "       [-0.00488379, -0.02837137, -0.02689171, ...,  0.02915828,\n",
      "         0.01987113, -0.04700942],\n",
      "       [-0.02499191, -0.03482578, -0.0376398 , ..., -0.02710248,\n",
      "        -0.05809933, -0.04310897],\n",
      "       [-0.00771295, -0.00076351,  0.00250044, ..., -0.05280703,\n",
      "        -0.04770042, -0.03610853]]), 'b': array([[ 4.05132856e-18],\n",
      "       [ 2.98601127e-17],\n",
      "       [-1.19772560e-17],\n",
      "       [ 5.73198104e-17],\n",
      "       [-5.79452409e-18],\n",
      "       [ 1.21780244e-17],\n",
      "       [-4.60820682e-17],\n",
      "       [ 1.72943437e-17],\n",
      "       [-6.64495508e-17],\n",
      "       [ 1.09350553e-17],\n",
      "       [ 1.12145077e-16],\n",
      "       [-3.28604680e-17],\n",
      "       [-3.05320030e-17],\n",
      "       [ 1.03653813e-16],\n",
      "       [ 1.66087422e-17],\n",
      "       [-3.77650744e-17],\n",
      "       [-6.32450630e-17],\n",
      "       [-5.05180433e-18],\n",
      "       [ 6.44289709e-17],\n",
      "       [ 7.97946673e-18],\n",
      "       [-4.51903394e-17],\n",
      "       [-1.26665241e-16],\n",
      "       [-3.23876136e-17],\n",
      "       [ 1.89249468e-17],\n",
      "       [-8.54261963e-17],\n",
      "       [-4.55254144e-17],\n",
      "       [ 3.39327256e-17],\n",
      "       [-1.89212989e-17],\n",
      "       [ 6.87195616e-17],\n",
      "       [-5.01285715e-17],\n",
      "       [-2.57932167e-17],\n",
      "       [ 4.65777482e-17],\n",
      "       [-4.73470332e-17],\n",
      "       [-2.08255877e-17],\n",
      "       [ 1.02449572e-16],\n",
      "       [-1.03307153e-16],\n",
      "       [ 5.58176106e-17],\n",
      "       [-6.17960872e-17],\n",
      "       [ 2.47420257e-17],\n",
      "       [ 4.33069826e-17],\n",
      "       [ 2.18397857e-17],\n",
      "       [ 5.87147879e-17],\n",
      "       [ 1.66467035e-17],\n",
      "       [ 4.48462363e-17],\n",
      "       [ 5.69711830e-17],\n",
      "       [-6.56592559e-17],\n",
      "       [ 9.95304291e-17],\n",
      "       [-2.38360879e-17],\n",
      "       [-2.24807015e-17],\n",
      "       [-1.09609664e-16]]), 'gamma': array([[0.78576134],\n",
      "       [1.07128138],\n",
      "       [1.01610778],\n",
      "       [1.38573048],\n",
      "       [0.89978198],\n",
      "       [0.98134862],\n",
      "       [0.93126781],\n",
      "       [0.86816381],\n",
      "       [0.88776051],\n",
      "       [1.02757898],\n",
      "       [1.48965121],\n",
      "       [0.77831362],\n",
      "       [0.83967275],\n",
      "       [1.55175164],\n",
      "       [0.86140723],\n",
      "       [1.01630484],\n",
      "       [1.02407925],\n",
      "       [0.86893241],\n",
      "       [2.90557925],\n",
      "       [0.90918094],\n",
      "       [1.43267778],\n",
      "       [0.77276951],\n",
      "       [0.97241922],\n",
      "       [0.89657497],\n",
      "       [1.0831489 ],\n",
      "       [0.7665645 ],\n",
      "       [1.1927471 ],\n",
      "       [1.03925175],\n",
      "       [0.99430966],\n",
      "       [0.86681838],\n",
      "       [0.96121012],\n",
      "       [1.0126982 ],\n",
      "       [1.05168777],\n",
      "       [1.15086517],\n",
      "       [1.05846075],\n",
      "       [1.06930588],\n",
      "       [1.17929772],\n",
      "       [0.90521713],\n",
      "       [0.79211214],\n",
      "       [0.93392462],\n",
      "       [1.06625821],\n",
      "       [1.13192979],\n",
      "       [0.77961761],\n",
      "       [1.05378001],\n",
      "       [0.85354191],\n",
      "       [1.81030504],\n",
      "       [0.95268272],\n",
      "       [0.81830534],\n",
      "       [1.11547037],\n",
      "       [1.08046109]]), 'beta': array([0.08516418]), 'mean': array([-0.03674443,  0.00180761, -0.06231103, -0.22647205,  0.07768415,\n",
      "        0.00562091, -0.30114321, -0.02361116, -0.22364786, -0.17236645,\n",
      "       -0.10486427, -0.14175867, -0.01022666, -0.06930186, -0.09365462,\n",
      "       -0.17263327, -0.10997612, -0.30117625,  0.06408258, -0.06918791,\n",
      "       -0.29818414, -0.12063366, -0.06635342, -0.13295499,  0.04733736,\n",
      "       -0.10297017, -0.09168905, -0.19603488,  0.08737633, -0.20626177,\n",
      "       -0.10927887, -0.24397671, -0.01807256, -0.2332642 , -0.05838972,\n",
      "       -0.10288612, -0.13968699, -0.22421083, -0.11058677, -0.15895113,\n",
      "       -0.00207197, -0.00758895, -0.06686914, -0.18612216, -0.04469699,\n",
      "       -0.05961787, -0.00351759, -0.12879688, -0.17630276, -0.10968101]), 'var': array([0.02210827, 0.02959878, 0.02125621, 0.04788851, 0.01799119,\n",
      "       0.02819594, 0.02641756, 0.01641185, 0.02762367, 0.02787732,\n",
      "       0.04093936, 0.01863058, 0.02096025, 0.0265424 , 0.02043145,\n",
      "       0.0259421 , 0.02866704, 0.02359421, 0.07126359, 0.0166529 ,\n",
      "       0.0410412 , 0.01644574, 0.02258683, 0.03285557, 0.02992126,\n",
      "       0.01647925, 0.03074083, 0.02592545, 0.02433499, 0.02305656,\n",
      "       0.02677679, 0.02717861, 0.02439157, 0.02750142, 0.02820359,\n",
      "       0.02748374, 0.02394424, 0.02120966, 0.01714401, 0.02097991,\n",
      "       0.03137783, 0.03314194, 0.01403376, 0.02632696, 0.01870882,\n",
      "       0.06180399, 0.02454611, 0.02100737, 0.02286932, 0.02482163])}\n"
     ]
    }
   ],
   "source": [
    "print(layers[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
