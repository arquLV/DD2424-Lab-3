{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        dataset = pickle.load(f, encoding='latin1') # Nxd(3072) (Nx (32x32x3))\n",
    "        X = np.transpose(dataset['data'] / 255.) # d x N\n",
    "        mean_X = np.mean(X, axis=1) # mean of each row (each feature mean)\n",
    "        std_X = np.std(X, axis=1)\n",
    "        X = X - np.matlib.repmat(mean_X, X.shape[1], 1).T\n",
    "        X = np.divide(X, np.matlib.repmat(std_X, X.shape[1], 1).T)\n",
    "        \n",
    "        y = np.array(dataset['labels'])\n",
    "        Y = np.transpose(np.eye(X.shape[1], np.max(y) + 1)[y]) # K x N\n",
    "        return X, Y, y\n",
    "\n",
    "def load_all(validation_size):\n",
    "    X_1, Y_1, y_1 = load_batch('data/data_batch_1')\n",
    "    X_2, Y_2, y_2 = load_batch('data/data_batch_2')\n",
    "    X_3, Y_3, y_3 = load_batch('data/data_batch_3')\n",
    "    X_4, Y_4, y_4 = load_batch('data/data_batch_4')\n",
    "    X_5, Y_5, y_5 = load_batch('data/data_batch_5')\n",
    "    \n",
    "    X = np.concatenate((X_1, X_2, X_3, X_4, X_5[:,:-validation_size]), axis=1)\n",
    "    Y = np.concatenate((Y_1, Y_2, Y_3, Y_4, Y_5[:,:-validation_size]), axis=1)\n",
    "    y = np.concatenate((y_1, y_2, y_3, y_4, y_5[:-validation_size]))\n",
    "    \n",
    "    X_valid = X_5[:,-validation_size:]\n",
    "    Y_valid = Y_5[:,-validation_size:]\n",
    "    y_valid = y_5[-validation_size:]\n",
    "    return X, Y, y, X_valid, Y_valid, y_valid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalize(scores, mean, variance):\n",
    "    return np.dot(np.pow(np.diag(variance + 1e-9), -0.5), (s - np.array([mean]).transpose()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(s):\n",
    "    exponent = np.exp(s)\n",
    "    return np.divide(exponent, np.sum(exponent, axis=0))\n",
    "\n",
    "def evaluate_classifier(X, layers):\n",
    "    num_layers = len(layers)\n",
    "    H = []\n",
    "    S = []\n",
    "    S_norm = []\n",
    "    \n",
    "    layer_means = []\n",
    "    layer_variances = []\n",
    "    \n",
    "    h_prev = X\n",
    "    \n",
    "    for i, layer in enumerate(layers):\n",
    "        if i == num_layers - 1:  # If last layer\n",
    "            P = softmax(np.dot(layer[\"W\"], h_prev) + layer[\"b\"]) # K x N\n",
    "            return H, P, S, S_norm, layer_means, layer_variances\n",
    "        else:\n",
    "            s = np.dot(layer[\"W\"], h_prev) + layer[\"b\"] # m x N\n",
    "#             S.append(s)\n",
    "            \n",
    "#             mean = np.mean(s, axis=1) # m len\n",
    "#             variance = np.mean(np.square(s - np.array([mean]).transpose()), axis=1) # m\n",
    "            \n",
    "#             layer_means.append(mean)\n",
    "#             layer_variances.append(variance)\n",
    "            \n",
    "#             gamma = 1\n",
    "#             beta = 0\n",
    "            \n",
    "#             normalized = batch_normalize(scores, mean, variance)\n",
    "#             S_norm.append(normalized)\n",
    "            \n",
    "#             transformed = np.multiply(gamma, normalized) + beta\n",
    "            \n",
    "            h = np.maximum(s, 0) # ReLU; m x N\n",
    "            H.append(h)\n",
    "            h_prev = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, Y, layers, lmb):\n",
    "    H, P = evaluate_classifier(X, layers)[:2]\n",
    "    n = np.sum(np.multiply(Y, P), axis=0)\n",
    "    cross_entropy = np.sum(-np.log(n))\n",
    "    \n",
    "    w_square_sum = 0\n",
    "    if lmb > 0:\n",
    "        for layer in layers:\n",
    "            w_square_sum += np.sum(np.diag(np.dot(layer[\"W\"].T, layer[\"W\"])))\n",
    "    return (cross_entropy / X.shape[1]) + (lmb * w_square_sum)\n",
    "\n",
    "def compute_gradients(X, Y, layers, lmb):\n",
    "    H, P, S, S_norm, layer_means, layer_variances = evaluate_classifier(X, layers)\n",
    "    G = -(Y - P)\n",
    "    Nb = X.shape[1] # batch size\n",
    "    \n",
    "    W_gradients = []\n",
    "    b_gradients = []\n",
    "    for i, layer in reversed(list(enumerate(layers))): # from last to first\n",
    "        if i > 0:\n",
    "            grad_W = np.divide(np.dot(G, H[i - 1].T), Nb) + (2 * lmb * layer[\"W\"]) # J w.r.t W_k\n",
    "            grad_b = np.divide(np.dot(G, np.ones((Nb, 1))), Nb) # J w.r.t b_k\n",
    "            \n",
    "            G = np.dot(layer[\"W\"].T, G)\n",
    "            G = G * (H[i - 1] > 0).astype(int) # element-wise\n",
    "            \n",
    "#             gamma_grad = np.dot(np.divide(np.multiply(G, S_norm), Nb), np.ones((Nb, 1)))\n",
    "#             beta_grad = np.divide(np.dot(G, np.ones((Nb, 1))))\n",
    "            \n",
    "#             G = np.multiply(G, np.dot()\n",
    "            \n",
    "            W_gradients.append(grad_W)\n",
    "            b_gradients.append(grad_b)\n",
    "        else: # first layer\n",
    "            grad_W = np.divide(np.dot(G, X.T), Nb) + (2 * lmb * layer[\"W\"])\n",
    "            grad_b = np.divide(np.dot(G, np.ones((Nb, 1))), Nb)\n",
    "            W_gradients.append(grad_W)\n",
    "            b_gradients.append(grad_b)\n",
    "    return W_gradients, b_gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(X, y, layers):\n",
    "    _, p = evaluate_classifier(X, layers)\n",
    "    argmax = np.argmax(p, axis=0) # max element index of each column\n",
    "    diff = argmax - y\n",
    "    return (diff == 0).sum() / X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_GD(X, Y, GDparams, layers, lmb, validation, calculate_loss=False):\n",
    "#     print(\"Training samples: {}\".format(X.shape[1]))\n",
    "#     print(\"Validation samples: {}\".format(validation[\"X\"].shape[1]))\n",
    "#     print(\"Training parameters: \", GDparams)\n",
    "    J_training = []\n",
    "    J_validation = []\n",
    "    \n",
    "    eta_diff = GDparams[\"eta_max\"] - GDparams[\"eta_min\"]\n",
    "    eta = GDparams[\"eta_min\"]\n",
    "    t = 0 # step\n",
    "    l = 0 # cycle\n",
    "    \n",
    "    runs_in_epoch = int(X.shape[1] / GDparams[\"n_batch\"])\n",
    "    # for epoch in range(GDparams[\"epochs\"]):\n",
    "    while l < GDparams[\"max_cycles\"]:\n",
    "        for j in range(1, runs_in_epoch):\n",
    "            j_start = (j - 1) * GDparams[\"n_batch\"]\n",
    "            j_end = j * GDparams[\"n_batch\"]\n",
    "            \n",
    "            X_batch = X[:, j_start:j_end]\n",
    "            Y_batch = Y[:, j_start:j_end]\n",
    "            \n",
    "            grad_W, grad_b = compute_gradients(X_batch, Y_batch, layers, lmb)\n",
    "            \n",
    "            for i, layer in enumerate(layers):\n",
    "                layer[\"W\"] = layer[\"W\"] - (eta * grad_W[-1 - i])\n",
    "                layer[\"b\"] = layer[\"b\"] - (eta * grad_b[-1 - i])\n",
    "        \n",
    "            if calculate_loss and t % 100 == 0:\n",
    "                J_training.append(compute_cost(X, Y, layers, lmb))\n",
    "                J_validation.append(compute_cost(validation[\"X\"], validation[\"Y\"], layers, lmb))\n",
    "                print(\"Step {}, training loss: {}\".format(t, J_training[-1]))\n",
    "                \n",
    "            t += 1 # next update step\n",
    "            if t % (2 * GDparams[\"n_s\"]) == 0:\n",
    "                l += 1 # next cycle\n",
    "                if l == GDparams[\"max_cycles\"]:\n",
    "                    break\n",
    "#                 print(\"Entering cycle {}, t: {}, eta: {}\".format(l, t, eta))\n",
    "            if t <= (2*l + 1) * GDparams[\"n_s\"]:\n",
    "                eta = GDparams[\"eta_min\"] + (eta_diff * ((t - (2 * l * GDparams[\"n_s\"])) / GDparams[\"n_s\"]))\n",
    "            else:\n",
    "                eta = GDparams[\"eta_max\"] - (eta_diff * (t - ((2*l + 1) * GDparams[\"n_s\"])) / GDparams[\"n_s\"])\n",
    "\n",
    "    if calculate_loss:\n",
    "        return layers, J_training, J_validation\n",
    "    else:\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_batch': 100, 'eta_min': 1e-05, 'eta_max': 0.1, 'n_s': 500, 'epochs': 10, 'max_cycles': 1}\n"
     ]
    }
   ],
   "source": [
    "X, Y, y = load_batch('data/data_batch_1')\n",
    "X_valid, Y_valid, y_valid = load_batch('data/data_batch_2')\n",
    "# X, Y, y, X_valid, Y_valid, y_valid = load_all(validation_size=1000)\n",
    "X_test, Y_test, y_test = load_batch('data/test_batch')\n",
    "\n",
    "d = X.shape[0]\n",
    "N = X.shape[1]\n",
    "K = Y.shape[0]\n",
    "\n",
    "# X: d x N\n",
    "# Y: K x N\n",
    "\n",
    "# m = 50 # hidden units\n",
    "\n",
    "# std_dev_1 = 1 / np.sqrt(d)\n",
    "# std_dev_2 = 1 / np.sqrt(m)\n",
    "# W_1 = std_dev * np.random.randn(m, d)\n",
    "# b_1 = std_dev * np.random.randn(m, 1)\n",
    "\n",
    "# W_2 = std_dev * np.random.randn(K, m)\n",
    "# b_2 = std_dev * np.random.randn(K, 1)\n",
    "def init_network(dimensions=d, num_layers=2, hidden_units=[50], classes=K):\n",
    "    \n",
    "    layers = [\n",
    "        {\n",
    "            \"W\": (1 / np.sqrt(dimensions)) * np.random.randn(hidden_units[0], dimensions),\n",
    "            \"b\": np.zeros((hidden_units[0], 1)),\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    if len(hidden_units) > 1:\n",
    "        for prev_i, m in enumerate(hidden_units[1:]):\n",
    "            layers.append({\n",
    "                \"W\": (1 / np.sqrt(hidden_units[prev_i])) * np.random.randn(m, hidden_units[prev_i]),\n",
    "                \"b\": np.zeros((m, 1)),\n",
    "            })\n",
    "        \n",
    "    layers.append({\n",
    "            \"W\": (1 / np.sqrt(hidden_units[num_layers-2])) * np.random.randn(classes, hidden_units[num_layers-2]),\n",
    "            \"b\": np.zeros((K, 1)),\n",
    "        })\n",
    "    \n",
    "    return layers\n",
    "\n",
    "\n",
    "lmb = 0.01  # lambda\n",
    "GDparams = {\n",
    "    \"n_batch\": 100,\n",
    "    \"eta_min\": 1e-5,\n",
    "    \"eta_max\": 1e-1,\n",
    "    \"n_s\": 500,\n",
    "    \"epochs\": 10,\n",
    "    \"max_cycles\": 1,\n",
    "}\n",
    "#     \"n_s\": 2 * np.floor(X.shape[1] / 100),\n",
    "validation = {\n",
    "    \"X\": X_valid,\n",
    "    \"Y\": Y_valid,\n",
    "}\n",
    "\n",
    "print(GDparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients_num(X, Y, layers, lmb, h):\n",
    "    \n",
    "    grad_W = [np.zeros(layer[\"W\"].shape) for layer in layers]\n",
    "    grad_b = [np.zeros(layer[\"W\"].shape[0]) for layer in layers]\n",
    "    c = compute_cost(X, Y, layers, lmb)\n",
    "    \n",
    "    for l, layer in enumerate(layers):\n",
    "        for i in range(len(layer[\"b\"])):\n",
    "            layers_try = copy.deepcopy(layers)\n",
    "            layers_try[l][\"b\"][i] = layers_try[l][\"b\"][i] + h\n",
    "            c2 = compute_cost(X, Y, layers_try, lmb)\n",
    "            grad_b[l][i] = (c2 - c) / h\n",
    "\n",
    "        W_shape = layer[\"W\"].shape\n",
    "        for i in range(W_shape[0]):\n",
    "            for j in range(W_shape[1]):\n",
    "                layers_try = copy.deepcopy(layers)\n",
    "                layers_try[l][\"W\"][i,j] = layers_try[l][\"W\"][i,j] + h\n",
    "                c2 = compute_cost(X, Y, layers_try, lmb)\n",
    "                grad_W[l][i,j] = (c2 - c) / h\n",
    "        \n",
    "    return grad_W, grad_b\n",
    "\n",
    "def compute_gradients_num_slow(X, Y, layers, lmb, h):\n",
    "    \n",
    "    grad_W = [np.zeros(layer[\"W\"].shape) for layer in layers]\n",
    "    grad_b = [np.zeros(layer[\"b\"].shape[0]) for layer in layers]\n",
    "\n",
    "    for l, layer in enumerate(layers):\n",
    "        for i in range(len(layer[\"b\"])):\n",
    "            layers_try1 = copy.deepcopy(layers)\n",
    "            layers_try2 = copy.deepcopy(layers)                                       \n",
    "                                       \n",
    "            layers_try1[l][\"b\"][i] = layers_try1[l][\"b\"][i] - h\n",
    "            c1 = compute_cost(X, Y, layers_try1, lmb)\n",
    "            \n",
    "            \n",
    "            layers_try2[l][\"b\"][i] = layers_try2[l][\"b\"][i] + h\n",
    "            c2 = compute_cost(X, Y, layers_try2, lmb)\n",
    "            \n",
    "            grad_b[l][i] = (c2 - c1) / (2*h)\n",
    "        \n",
    "        W_shape = layer[\"W\"].shape \n",
    "        for i in range(W_shape[0]):\n",
    "            for j in range(W_shape[1]):\n",
    "                layers_try1 = copy.deepcopy(layers)\n",
    "                layers_try2 = copy.deepcopy(layers) \n",
    "\n",
    "                layers_try1[l][\"W\"][i,j] = layers_try1[l][\"W\"][i,j] - h\n",
    "                c1 = compute_cost(X, Y, layers_try1, lmb)\n",
    "                \n",
    "                layers_try2[l][\"W\"][i,j] = layers_try2[l][\"W\"][i,j] + h\n",
    "                c2 = compute_cost(X, Y, layers_try2, lmb)\n",
    "                grad_W[l][i,j] = (c2 - c1) / (2*h)\n",
    "        \n",
    "    return grad_W, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "grad b (last to first):\n",
      "[[0.00000000e+00 4.94719453e-10 0.00000000e+00 2.23470375e-09\n",
      "  0.00000000e+00 4.49989568e-10 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 2.34395266e-10 1.39301063e-10\n",
      "  3.57721164e-10 8.29952460e-10 6.77770968e-11 0.00000000e+00\n",
      "  1.51499017e-10 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "Layer 4 mean diff: 2.4800294187506034e-10\n",
      "[[0.00000000e+00 6.77545235e-10 0.00000000e+00 0.00000000e+00\n",
      "  1.19014249e-09 1.01316313e-10 2.06984228e-10 0.00000000e+00\n",
      "  1.75638534e-10 0.00000000e+00 4.34733307e-10 1.94624255e-08\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.24209451e-11\n",
      "  0.00000000e+00 1.00468838e-10 0.00000000e+00 0.00000000e+00]]\n",
      "Layer 3 mean diff: 1.1220837718285377e-09\n",
      "[[1.91256076e-10 2.08990307e-10 1.61221247e-10 2.40845168e-10\n",
      "  1.80296044e-10 0.00000000e+00 3.25959237e-10 7.38770597e-11\n",
      "  0.00000000e+00 7.18533570e-11 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.52598674e-10 0.00000000e+00]]\n",
      "Layer 2 mean diff: 8.034485850364922e-11\n",
      "[[1.07609744e-10 5.48705328e-10 1.24133177e-10 1.03699121e-10\n",
      "  5.93020014e-10 1.52210841e-10 4.31998411e-11 3.53397203e-10\n",
      "  6.70671082e-10 9.87991640e-11]]\n",
      "Layer 1 mean diff: 2.795445515175106e-10\n",
      "\n",
      "grad W (last to first):\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [2.06267871e-10 2.04263122e-10 2.27727915e-10 3.82956299e-10\n",
      "  2.42659613e-10 1.22537479e-09 2.95576031e-09 2.46275371e-09\n",
      "  5.79752493e-09 4.59507061e-09]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [2.81279459e-10 1.03552634e-09 1.18528279e-09 2.21415870e-09\n",
      "  1.98561809e-09 4.44000496e-09 6.84628875e-08 4.69687073e-08\n",
      "  1.11111037e-08 1.00917497e-08]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.86610854e-10 5.72078394e-11 1.68165705e-09 5.36574854e-10\n",
      "  3.27796326e-09 3.63853903e-09 1.51967303e-08 9.59259356e-09\n",
      "  4.32798691e-09 6.87874169e-09]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.10447929e-09 5.91498418e-10 4.16244790e-10 2.02428122e-10\n",
      "  2.32905288e-09 9.92348745e-10 6.66946090e-09 8.82746878e-10\n",
      "  4.79912712e-09 1.87873689e-09]\n",
      " [1.24377952e-09 3.01946563e-12 1.34865608e-09 3.99630206e-10\n",
      "  1.85719559e-09 1.13092652e-09 1.01749962e-08 4.69016487e-09\n",
      "  4.02215975e-09 4.19844366e-09]\n",
      " [5.87468566e-10 2.09531664e-10 5.34981012e-10 3.14681576e-10\n",
      "  3.30662412e-10 4.77908507e-10 1.88188575e-09 2.11754311e-09\n",
      "  3.19199834e-10 7.11718621e-10]\n",
      " [7.51320874e-11 1.72490752e-09 1.03613349e-09 1.84037302e-09\n",
      "  5.03214024e-09 3.59446312e-09 5.00491356e-09 6.25118675e-09\n",
      "  9.59765123e-10 6.10229549e-09]\n",
      " [2.58980455e-09 2.92057350e-10 3.57300953e-10 2.65351619e-10\n",
      "  1.97162871e-09 3.78688831e-09 1.79487144e-08 1.35359852e-08\n",
      "  3.48069595e-09 6.79273940e-09]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.54128503e-09 2.92032377e-09 9.27572799e-10 3.22779665e-10\n",
      "  2.16956816e-09 9.03930323e-09 1.12780580e-08 4.08001147e-09\n",
      "  4.32251470e-09 2.21473818e-08]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "Layer 4 mean diff: 2.0573161766096295e-09\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 3.21346398e-10 0.00000000e+00 2.25521812e-10\n",
      "  0.00000000e+00 2.24197555e-09 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 8.07496485e-10 1.00198739e-09\n",
      "  7.56894119e-10 2.11498695e-10 2.94545961e-10 0.00000000e+00\n",
      "  5.02975873e-10 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 6.44794154e-10 0.00000000e+00 1.06835021e-09\n",
      "  0.00000000e+00 5.12825677e-09 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 7.02292109e-10 1.68218018e-09\n",
      "  3.77663958e-09 2.60798245e-09 5.06021707e-10 0.00000000e+00\n",
      "  7.95429575e-10 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 9.14691891e-10 0.00000000e+00 7.06909114e-10\n",
      "  0.00000000e+00 2.04833190e-09 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.70966005e-09 6.99433455e-10\n",
      "  1.07622003e-09 3.90373695e-10 9.21963087e-10 0.00000000e+00\n",
      "  2.33965251e-09 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 2.33440667e-10 0.00000000e+00 3.80097554e-10\n",
      "  0.00000000e+00 2.57622405e-10 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 2.58026002e-10 3.92648890e-10\n",
      "  6.89774389e-10 1.25458052e-10 8.16165997e-11 0.00000000e+00\n",
      "  2.71165110e-11 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.14005942e-10 0.00000000e+00 5.59165880e-10\n",
      "  0.00000000e+00 1.74323178e-09 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.94434752e-10 4.08742954e-10\n",
      "  6.21321344e-10 6.74276955e-10 1.12302592e-10 0.00000000e+00\n",
      "  5.55349285e-10 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.12333433e-09 0.00000000e+00 9.09906613e-10\n",
      "  0.00000000e+00 3.46307263e-09 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 8.42170688e-10 2.45808180e-10\n",
      "  1.10452861e-10 1.31469082e-10 3.56783837e-10 0.00000000e+00\n",
      "  3.23114659e-10 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.08277225e-08 0.00000000e+00 4.48029257e-08\n",
      "  0.00000000e+00 9.16864875e-08 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 2.04494893e-08 5.04525434e-08\n",
      "  5.69542269e-08 2.60749031e-09 2.37625524e-08 0.00000000e+00\n",
      "  7.65187158e-08 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.66452081e-09 0.00000000e+00 4.86146516e-10\n",
      "  0.00000000e+00 5.69846800e-09 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 2.50589831e-09 2.86424284e-10\n",
      "  3.81348847e-10 3.14259285e-09 1.68837498e-09 0.00000000e+00\n",
      "  4.41739516e-09 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 2.05703057e-11 0.00000000e+00 4.68111418e-10\n",
      "  0.00000000e+00 1.24483831e-09 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.03730865e-10 6.46187861e-10\n",
      "  3.05484863e-10 3.26462479e-10 3.90307071e-10 0.00000000e+00\n",
      "  9.83116589e-10 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "Layer 3 mean diff: 1.1301007586420425e-09\n",
      "[[0.00000000e+00 4.80406114e-08 0.00000000e+00 0.00000000e+00\n",
      "  1.73188104e-08 6.77839993e-09 1.45915929e-09 0.00000000e+00\n",
      "  2.36585372e-09 0.00000000e+00 1.52540869e-09 2.79926591e-08\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 6.08520484e-10\n",
      "  0.00000000e+00 1.43073938e-09 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 7.25598447e-09 0.00000000e+00 0.00000000e+00\n",
      "  2.15042033e-09 1.08187960e-09 3.29751142e-10 0.00000000e+00\n",
      "  8.12070935e-10 0.00000000e+00 3.83358301e-11 9.26313455e-10\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.14829442e-10\n",
      "  0.00000000e+00 4.31804163e-10 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.51593526e-09 0.00000000e+00 0.00000000e+00\n",
      "  1.23050226e-09 8.18465736e-10 2.23099124e-10 0.00000000e+00\n",
      "  1.38236103e-09 0.00000000e+00 1.53285700e-09 2.84017254e-09\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 8.99095885e-11\n",
      "  0.00000000e+00 4.81965994e-10 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 6.43735739e-10 0.00000000e+00 0.00000000e+00\n",
      "  3.55382442e-09 2.89680298e-09 2.97899433e-10 0.00000000e+00\n",
      "  1.05541053e-09 0.00000000e+00 1.03914948e-10 2.00146678e-09\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.64010425e-09\n",
      "  0.00000000e+00 4.59285737e-10 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 9.32657986e-09 0.00000000e+00 0.00000000e+00\n",
      "  6.12623975e-09 3.59683817e-10 1.33002132e-10 0.00000000e+00\n",
      "  1.07882677e-09 0.00000000e+00 3.84932146e-10 3.90068834e-09\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.38999542e-09\n",
      "  0.00000000e+00 4.49702507e-10 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.11669581e-08 0.00000000e+00 0.00000000e+00\n",
      "  5.74127142e-09 1.03847998e-09 1.47536864e-10 0.00000000e+00\n",
      "  2.05966736e-09 0.00000000e+00 3.77841184e-10 4.13279353e-10\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.94485317e-11\n",
      "  0.00000000e+00 1.25739185e-10 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.17142951e-09 0.00000000e+00 0.00000000e+00\n",
      "  1.20936332e-09 1.42167826e-10 4.26824060e-10 0.00000000e+00\n",
      "  6.00740192e-10 0.00000000e+00 6.89846671e-11 2.84253990e-09\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.13693721e-10\n",
      "  0.00000000e+00 5.41079320e-10 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 8.60131978e-10 0.00000000e+00 0.00000000e+00\n",
      "  1.32311720e-09 1.09269020e-09 2.28502095e-10 0.00000000e+00\n",
      "  1.15431230e-09 0.00000000e+00 2.04813943e-10 3.36673614e-09\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 6.63744481e-10\n",
      "  0.00000000e+00 4.80861349e-10 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.21583277e-09 0.00000000e+00 0.00000000e+00\n",
      "  7.17641246e-09 3.15099042e-09 1.14588503e-10 0.00000000e+00\n",
      "  4.60631539e-10 0.00000000e+00 2.71801403e-09 1.31212960e-08\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.23719558e-11\n",
      "  0.00000000e+00 3.34511914e-09 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "Layer 2 mean diff: 5.892903171411284e-10\n",
      "[[6.98448251e-10 2.47887837e-09 1.40158699e-09 6.10260808e-09\n",
      "  4.47234614e-10 0.00000000e+00 3.22072756e-09 8.83128167e-08\n",
      "  0.00000000e+00 2.24316011e-10 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.80207776e-09 0.00000000e+00]\n",
      " [1.69516954e-09 3.47159615e-09 4.69638159e-09 4.81541936e-09\n",
      "  9.55417072e-10 0.00000000e+00 3.97903417e-09 7.53198401e-08\n",
      "  0.00000000e+00 1.75493435e-09 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 7.43597827e-09 0.00000000e+00]\n",
      " [3.44252942e-09 6.52112327e-09 3.38255205e-10 1.57376424e-09\n",
      "  1.33668941e-09 0.00000000e+00 1.57922312e-09 4.38951465e-08\n",
      "  0.00000000e+00 1.04185837e-09 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.96061999e-08 0.00000000e+00]\n",
      " [7.24855119e-10 9.72614455e-10 1.71627370e-10 8.29409514e-09\n",
      "  2.05805517e-09 0.00000000e+00 1.21585627e-09 6.56541358e-08\n",
      "  0.00000000e+00 6.38001766e-10 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.60626343e-09 0.00000000e+00]\n",
      " [3.89460752e-10 5.34466300e-09 1.50156409e-09 1.70286892e-08\n",
      "  5.33623735e-10 0.00000000e+00 2.07708027e-09 1.68951879e-07\n",
      "  0.00000000e+00 2.33865430e-10 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 2.50977228e-08 0.00000000e+00]\n",
      " [7.78184978e-11 4.25143059e-09 5.46493232e-12 1.22298869e-08\n",
      "  4.74150316e-10 0.00000000e+00 1.50784517e-09 1.12440016e-07\n",
      "  0.00000000e+00 2.42655859e-09 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 4.14458086e-10 0.00000000e+00]\n",
      " [3.26725017e-11 8.20330092e-10 3.70328916e-10 9.48690648e-10\n",
      "  1.72577521e-10 0.00000000e+00 2.03025561e-10 7.82888860e-09\n",
      "  0.00000000e+00 1.71772077e-10 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 2.44315148e-09 0.00000000e+00]\n",
      " [3.01120135e-09 4.65452146e-09 2.54503199e-09 2.30137534e-09\n",
      "  6.43130969e-10 0.00000000e+00 1.10574849e-09 1.19852216e-07\n",
      "  0.00000000e+00 2.66196241e-09 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.93669628e-08 0.00000000e+00]\n",
      " [7.29849724e-10 3.67098182e-09 4.36410206e-10 9.20138945e-10\n",
      "  8.52202103e-10 0.00000000e+00 1.01782360e-09 1.30171971e-07\n",
      "  0.00000000e+00 1.11635529e-09 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.20974735e-08 0.00000000e+00]\n",
      " [4.97925470e-10 1.17836148e-09 1.83336571e-09 4.60015182e-09\n",
      "  1.74584825e-10 0.00000000e+00 1.84156215e-09 9.62665018e-08\n",
      "  0.00000000e+00 8.50974359e-11 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 9.19283483e-10 0.00000000e+00]]\n",
      "Layer 1 mean diff: 5.770062829901377e-09\n"
     ]
    }
   ],
   "source": [
    "feature_dims = 10\n",
    "samples = 1\n",
    "\n",
    "layers_g = init_network(dimensions=feature_dims, num_layers=4, hidden_units=[20,20,20], classes=K)\n",
    "X_g = X[0:feature_dims, 0:samples] #d X N\n",
    "Y_g = Y[:, 0:samples] #K x N\n",
    "\n",
    "grad_W, grad_b = compute_gradients(X_g, Y_g, layers_g, lmb=0)\n",
    "ngrad_w, ngrad_b = compute_gradients_num_slow(X_g, Y_g, layers_g, lmb=0, h=1e-6)\n",
    "print(\"\\ngrad b (last to first):\")\n",
    "\n",
    "for i in range(len(grad_b)):\n",
    "    diffs = np.abs(grad_b[len(grad_b) - 1 - i].T - ngrad_b[i]) / np.maximum(1e-6, np.abs(grad_b[len(grad_b) - 1 - i].T) + np.abs(ngrad_b[i]))\n",
    "    print(diffs)\n",
    "    print(\"Layer {} mean diff: {}\".format(len(grad_b) - i, np.mean(diffs)))\n",
    "\n",
    "\n",
    "print(\"\\ngrad W (last to first):\")\n",
    "for i in range(len(grad_W)):\n",
    "    diffs = np.abs(grad_W[len(grad_W) - 1 - i] - ngrad_w[i]) / np.maximum(1e-6, np.abs(grad_W[len(grad_W) - 1 - i]) + np.abs(ngrad_w[i]))\n",
    "    print(diffs)\n",
    "    print(\"Layer {} mean diff: {}\".format(len(grad_W) - i, np.mean(diffs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
